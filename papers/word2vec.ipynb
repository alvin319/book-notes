{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "#### Authors: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Page 1\n",
    "#### Introduction\n",
    "- Skip-gram is an efficient method for learning vector representations\n",
    "- Word2vec offers an extension to improve the method by the following\n",
    "    - Subsampling frequent words => increase speed up\n",
    "    - Negative sampling\n",
    "- Distributed representation helps NLP by grouping similar words\n",
    "- Mikolov also introduced skip-gram model\n",
    "    - Not requiring dense matrix multplication\n",
    "    - Single machine with 100 billion words in 1 day\n",
    "- Example\n",
    "    - \"Madrid\" - \"Spain\" + \"France = \"Paris\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page 2\n",
    "- Subsampling frequents words => 2x ~ 10x speed up + accuracy improvement\n",
    "- Simplified varaint of Noise Contrastive Estimation (NCE) => faster training + better vector representaiton for frequent words\n",
    "    - In comparison to more complex hierarchical softmax\n",
    "- Word representation doesn't work well with idiomatic phrases\n",
    "    - So just use word vectors to represent phrases tend to work pretty well\n",
    "    - Find a bunch of phrases\n",
    "    - Treat the phrases as individual tokens\n",
    "\n",
    "#### Skip Gram Model\n",
    "- Objective is to find word representation that are useful for predicting the surrounding words in a sentence or a document\n",
    "- Given a sequence of training words $w_1, w_2, w_3, ..., w_T$, the model is to maximize the average log probability $$\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-c \\le j \\le c, j \\ne 0}log (P(w_{t+j}|w_t)) $$ where $c$ is the size of the training context\n",
    "    - Larger $c$ results in more training examples and thus can lead to a higher accuracy, but there's a time\n",
    "- The basic skip-gram formualtion defins $P(w_{t+j}|w_t)$ using softmax function\n",
    "    - $$P(w_O|w_I) = \\frac{exp(v`_{w_O}^{T}v_{w_I})}{\\sum_{w=1}^{W}exp(v`_w^{T}v_{w_I})}$$ where $v_w$ is the input vector representation of $w$ and $v`_w$ is the output representation\n",
    "    - Thi is pretty much impractical because the computation cost of the gradient is expensive (proportional to W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page 3\n",
    "#### Hierarchical Softmax\n",
    "- Efficient approximation of the full softmax\n",
    "- Instead of evaluating W output nodes like softmax (multi-class classification), it just needs to evaluate only about $log_2(W)$ nodes\n",
    "    - To achieve that, it uses a binary tree repsentation of the output layer with W nodes as its leaves, and for each node, explicityly represents the relative probabilities of its child nodes\n",
    "- $P(w|w_I) = \\prod_{j = 1}^{L(w) - 1} \\sigma([[n(w, j + 1) = ch(n(w, j))]] \\cdot v`_{n(w, j)}^{T}v_{w_I})$\n",
    "    - Each word $w$ can be reached by a path from root of the tree. \n",
    "    - $n(w, j)$ = jth node on the path from root to $w$\n",
    "    - $L(w)$ is the length of the path\n",
    "    - $n(w, 1)$ = root and $n(w, L(w)) = w$\n",
    "    - $ch(n)$ = an arbitrary fixed child of n\n",
    "    - $[[x]] = 1$ if $x$ is true and $-1$ otherwise\n",
    "    - $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "- Now the gradient is proportional to $L(W_O)$\n",
    "- Unlike the standard softmax formulation of skipgram which one representation of $v_w$ for each word $w$w and one representation of $v`_n$ for every innde node $n$ of the binary tree\n",
    "- Using binary tree can speed up on performance, and in their work, they have utilized huffman tree to implement this idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page 4\n",
    "#### Negative Sampling\n",
    "- Alternative to the hierachical soft is Noise Contrastive Estimation (NCE)\n",
    "- NCE can approximately maximize the log probability of softmax, the skipgram model is only concerned with learning high quality vector representations, so we are free to simplify NCE as long as the vector representations retain their quality\n",
    "- Negative sampling = $log \\sigma(v`_{w_O}^{T}v_{w_I}) + \\sum_{i=1}^{k} \\mathop{\\mathbb{E}_{w_i ~ P_n(w)}}[log \\sigma(-v`_{w_i}^{T}v_{w_I})]$ which is used to replace every $log P(w_O|w_I)$ term in the skip gram objective\n",
    "- Thus the task is to distinguish the target word $w_O$ from draws from the noise distribution $P_n(w)$ using logistic regression, where there are $k$ negative samples for each data sample\n",
    "\n",
    "#### Subsmapling of frequent words\n",
    "- In a large corpora, the most frequent words can esaily occur hundreds of millions of times. Such words are pretty useless and proivide less information value\n",
    "- The vector representations of frequent words do not change significantly after training on several million examples\n",
    "- To counter this imbalance, each word $w_i$ in the training set is discarded with the probability computed by the formula $P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$\n",
    "    - $f(w_i)$ is the frequency of the word $w_i$\n",
    "    - $t$ is a chosen threshold, typically around $10^{-5}$\n",
    "- This formula aggressively subsamples words whose frequency is greater than $t$ while preserving the ranking of frequencies\n",
    "- This is a heuristic but it works well in practice and it accelerates learning and significantly improves the accuracy of the learned vectors of the rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page 5\n",
    "#### Empirical Result\n",
    "- Evaluating hierarchical softmax, noise contrastive estimation, negative sampling, and subsampling of the training words\n",
    "- Result shows that negative sampling outperforms the hierarchical softmax and slightly better than NCE\n",
    "- Utilizing $10^{-5}$ subsampling of frequent words improve the training speed by a good amount and also improve a good chunk of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page 6\n",
    "#### Learning phrases\n",
    "- To learn the vector representations for phrases, first find words that appear frequently together\n",
    "- phrases are formed based on the unigram and bigram counts using the following\n",
    "    - $score(w_i, w_j) = \\frac{count(w_iw_j) - \\sigma}{count(w_i) * count(w_j)}$\n",
    "        - $\\sigma$ is used as a discounting coefficient and prevents too many phrases consisting of very infrequent words to be formed\n",
    "    - Bigrams with score above the chosen threshold are then used as phrases\n",
    "- Typically, run 2 ~ 4 passes over the training data with decresing threshold value, allow longer phrases to be formed\n",
    "#### Phrase Skip-Gram Results\n",
    "- Hierarchical softmax performs the best when subsampling is happening\n",
    "- Large of amount of training data is crucial for the accuracy (72% to 66% drop from 33B words to 6B words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page 7\n",
    "#### Additive Compositionality\n",
    "- Skip gram model can perform precise analogical reasnoning using simple vector arithmetics\n",
    "- The word vectors are in a linear relaitonship with the inputs to the softmax nonlinearity\n",
    "- As the word vectors are trained to predict the surrounding words in the sentence, the vectors can be seen as representing the distribution of the context in which a word appears\n",
    "- These values are related logarithmically to the probabilties computed by the output layer, therefore the sum of two word vectors is related to the product of the two context distributions (the product works as an AND function)\n",
    "- Words that are assigned high probabilities by both word vectors will have high probability, and other words will have low probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page 8\n",
    "#### Comparison to other word representations & Conclusion\n",
    "- Skip gram is the best one vs Collobert and Weston, Turian, and Mnih and Hinton\n",
    "- Breakthrough in terms of word representation\n",
    "- Choosing hyper-parameter and algorithm is a task specific decision\n",
    "- Skip gram is able to train on several orders of magnitude more data than the previously published models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra notes\n",
    "- The way to generate the vector for a word (training in general)\n",
    "    - Initialize vectors with random weights\n",
    "    - For each element in the vector (the size is adjustable)\n",
    "    - Essentially running the softmax to get a probability of your $P(w_{t+j}|w_t)$, and that's 1 element of the vector\n",
    "    - Therefore, the vector just represent a distribution of probabilities of your surrounding words\n",
    "    - From the steps above, that's just 1 training for a word vector\n",
    "    - Now, run through the corpiuis to train the model (no gradient descent involved) for 1 iteration, and this is an iterative algorithm\n",
    "    - AKA skip gram\n",
    "- Optimization on regular standard softmax\n",
    "    - Hierachical softmax uses a binary tree to reduce time/space complexity, but also it's a good approximation\n",
    "- Optimization on skip gram\n",
    "    - Using negative sampling to replace the $logP(W_O|W_t)$ term to approximate the probability\n",
    "- Optimization on general accuracy\n",
    "    - Using subsampling to improve accuracy + training speed\n",
    "        - The word frequency in the denominator is similar to TFIDF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
